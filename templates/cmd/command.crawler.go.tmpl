package cmd

import (
	"context"
	"{{ .ModuleName }}/config"
	"{{ .ModuleName }}/db/postgres"
{{- range $index, $target := .Crawler.Targets }}
	"{{ $.ModuleName }}/{{ .Service | snakecase }}"
{{- end }}
	"os"
	"os/signal"
	"strings"
	"time"
	"sync"

	"github.com/antchfx/htmlquery"
	"github.com/labstack/gommon/log"
	"github.com/spf13/cobra"
	"golang.org/x/net/html"
)

const (
	postDelayPerCrawl   = {{ .Crawler.PostDelayPerCrawl }} * time.Hour
	postDelayPerRequest = {{ .Crawler.PostDelayPerRequest }} * time.Second
	workerCount = {{ .Crawler.WorkerCount }}
	baseURL     = "{{ .Crawler.BaseURL }}"
)

// crawlerCmd represents the crawler command
var crawlerCmd = &cobra.Command{
	Use:   "crawler",
	Short: "Start the crawler",
	Long:  `Start the crawler`,
	Run:   RunCrawler,
}

func init() {
	rootCmd.AddCommand(crawlerCmd)
}

{{- range $index, $target := .Crawler.Targets }}
type {{ .Name }} struct {
	{{- range $index, $property := .Properties }}
    {{ .Name | title }} {{ .Type }}
    {{- end }}    
}
{{- end }}

type SafeQueue struct {
	mutex   sync.Mutex
	items   []string
	checked map[string]interface{}
}

func (q *SafeQueue) IsEmpty() bool {
	return len(q.items) == 0
}

func (q *SafeQueue) Check(value string) {
	q.mutex.Lock()
	q.checked[value] = true
	q.mutex.Unlock()
}

func (q *SafeQueue) Enqueue(value string) {
	if c := q.checked[value]; c != nil {
		log.Printf("URL already checked %s", value)
		return
	}
	q.mutex.Lock()
	q.items = append(q.items, value)
	q.mutex.Unlock()
}

func (q *SafeQueue) Dequeue() *string {
	if len(q.items) == 0 {
		return nil
	}

	q.mutex.Lock()
	value := q.items[0]
	q.items = q.items[1:]
	q.mutex.Unlock()
	return &value
}

func crawl(
    worker int, 
    url string, 
    urlQueue *SafeQueue,
    {{- range $index, $target := .Crawler.Targets }}
    {{ .Name | untitle }}Service *{{ .Name | untitle }}.Service,
    {{- end }}
) {
	log.Printf("crawling %s", url)
	doc, err := htmlquery.LoadURL(url)
	if err != nil {
		log.Errorf("error loading url: %s %v", url, err)
		return
	}

	go extractURLs(url, doc, urlQueue)
    {{- range $index, $target := .Crawler.Targets }}
	go extract{{ .Name }}(url, doc, {{ .Name | untitle }}Service)
    {{- end }}
}

func extractURLs(
    url string, 
    doc *html.Node, 
    urlQueue *SafeQueue,
) {
	urls := htmlquery.Find(doc, "//a[@href]")

	if len(urls) == 0 {
		log.Printf("no URL found for %s", url)
		return
	}

	for _, node := range urls {
		href := htmlquery.SelectAttr(node, "href")
		if strings.HasPrefix(href, "/") {
			href = baseURL + href
		}
		
		if strings.HasPrefix(href, baseURL) {
			urlQueue.Enqueue(href)
		} else {
			log.Printf("ignore external URL: %s", href)
		}
	}
}

{{- range $index, $target := .Crawler.Targets }}
func extract{{ .Name }}(
    url string, 
    doc *html.Node,
    {{ .Name | untitle }}Service *{{ .Name | untitle }}.Service,
) {
	log.Print("Extracting {{ .Name }}")
    
    {{- range $index, $property := .Properties }}
	{{ .Name }} := htmlquery.FindOne(doc, "{{ .XPath }}")
    {{- end }}

	{{- range $index, $property := .Properties }}
	if {{ .Name }} == nil {
		log.Printf("invalid {{ .Name }}")
		return
	}
    {{- end }}

	item := {{ $target.Name }}{
    {{- range $index, $property := .Properties }}
		{{ .Name | title }}:   htmlquery.InnerText({{ .Name }}),
	{{- end }}
	}

	err := {{ .Name | untitle }}Service.Create{{ .Name }}Internal(
		context.Background(), 
		{{- range $index, $property := .Properties }}
		item.{{ .Name | title }},
		{{- end }}
	)

	if err != nil {
		log.Errorf("cannot create {{ .Name }}: %v", err)
	}
}
{{- end }}

func RunCrawler(cmd *cobra.Command, args []string) {
	config := config.Init()
	postgresDB := postgres.Init(config)

    {{- range $index, $target := .Crawler.Targets }}
	{{ .Name | untitle }}Repo := {{ .Name | untitle }}.NewRepo(postgresDB)
	{{ .Name | untitle }}Service := {{ .Name | untitle }}.NewService(&{{ .Name | untitle }}Repo)
    {{- end }}

    // TODO: Replace with the URL to start crawling
	startURL := baseURL + "/"

	go func() {
		for {
			log.Printf("Start crawling with %s", startURL)
			urlQueue := SafeQueue{items: []string{}, checked: map[string]interface{}{}}
			urlQueue.Enqueue(startURL)

			for {
				if urlQueue.IsEmpty() {
					log.Print("Finish crawling. Sleep.")
					time.Sleep(postDelayPerCrawl)
					break
				}

				for i := 0; i < workerCount; i++ {
					url := urlQueue.Dequeue()
					if url == nil {
						log.Print("URL list is empty")
						break
					}

					urlQueue.Check(*url)

					go crawl(
						i+1,
						*url,
						&urlQueue,
					{{- range $index, $target := .Crawler.Targets }}
						{{ .Name | untitle }}Service,
					{{- end }}
					)

					time.Sleep(postDelayPerRequest)
				}
			}
		}
	}()

	quit := make(chan os.Signal, 1)
	signal.Notify(quit, os.Interrupt)
	<-quit

	log.Print("Shutting down")
}
